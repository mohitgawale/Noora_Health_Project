# ğŸ“Š Noora Health Data Pipeline

This project is an end-to-end data pipeline that extracts WhatsApp-based **message** and **status** data from Google Sheets, loads it into a **PostgreSQL** database, applies **data transformations** and **validations**, and generates insightful **visualizations** using `matplotlib` and `seaborn`.

---

## ğŸ“ Project Structure

```
Noora_Health_Main/
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ credentials.ini               # GDrive and PostgreSQL credentials
â”‚   â””â”€â”€ key-<gdrive>.json             # Google Service Account key
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile                    # Docker setup for PySpark + Java
â”‚   â”œâ”€â”€ docker-compose.yml           # Docker Compose for PostgreSQL + app
â”‚   â””â”€â”€ requirements.txt             # Python package requirements
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ client.py                     # GoogleSheetClient & PostgresClient classes
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ create_table.py               # PostgreSQL table creation logic
â”‚   â”œâ”€â”€ ingest_data.py                # Loads data from Google Sheets to DB
â”‚   â”œâ”€â”€ transform.py                  # SQL-based data transformation
â”‚   â”œâ”€â”€ validation.py                 # Data quality checks
â”‚   â””â”€â”€ visualization.py             # Generates charts with matplotlib/seaborn
â”‚
â”œâ”€â”€ pipelineRunner.py                # Main script to orchestrate the full pipeline
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/your-org/noora-project.git
cd noora-project
```

### 2. Add Credentials

Place your credentials inside the `config/` directory:

- `credentials.ini` â€” defines PostgreSQL and Google Sheets API credentials
- `key-<project>.json` â€” Google Service Account key file

---

## ğŸ³ Build and Start Docker

```bash
cd docker/
docker-compose up --build
```

This will:

- Set up a PostgreSQL instance
- Run a PySpark container with required dependencies

---

## ğŸ§ª Run the Pipeline

You can execute the full pipeline using:

```bash
python pipelineRunner.py
```

This will:

- Create necessary PostgreSQL tables
- Ingest data from Google Sheets
- Transform and join the tables using SQL
- Run validation queries
- Generate summary visualizations

---

## ğŸ›  Technologies Used

- **Python 3.10**
- **PostgreSQL 14**
- **Docker + Docker Compose**
- `gspread` â€“ for Google Sheets API integration
- `psycopg2` â€“ for PostgreSQL DB interaction
- `pandas`, `matplotlib`, `seaborn` â€“ for data processing and visualization

---
